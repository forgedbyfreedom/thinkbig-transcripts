So let's go back to brain computer interface. How would you explain this to somebody at a party if they said, but it sounds pretty high tech, but what is it? OK. Well, OK. So let's just break apart the terms. Brain refers to really any kind of thing that interfaces with the cortex or the deeper structures. The computer is a digital device on the outside. A lot of people now call this BCI, brain computer, and it's a very messy term because it could mean a lot of different things. But I think it a nutshell what it means is for most people, a system that is recording from the brain, whether it's non-abacive from the scalp or something that's fully invasive within the brain itself, and connecting those signals to a computer that analyzes the signal and then does something with it. In many cases of BCI research, the application is, for example, to remove a computer cursor. Or in the research that we've done is to replace speech words for someone who's severely paralyzed and unable to talk anymore. And so it's about interpreting brain signals and then using a computer to interpret those signals and then transform them into a form that's useful to us. So in that example you gave your describing a patient with a phasia who can't speak. Let me be very specific about that. So a lot of the work that we've done is on people that have severe form of paralysis. And a phasia we typically refer to as someone who's got, let's say, a stroke in the language centers of the brain, where we've focused recently is on patients that have a severe form of paralysis like ALS. So they're the problem is they have largely normal language, but they can't get those. And get the motor signal out. And get the motor signal out to the vocal tract, the lips, the tone, the jaw, the larynx. Those descending fibers are severely affected by ALS, they degenerate. And that's why people progressively become paralyzed and loseability to speak. And an important part of that is that they loseability to speak, but they still have full cognition. Yeah. And for that individual by attaching a computer to their brain, you're able to hopefully extract in written cursive text, whatever across the computer screen, what they're wishing to say, that's right. OK, let's talk about how that could possibly be done. You mentioned earlier, there are at least two broad ways to extract that information, a non-invasive way where, presumably, you're putting electrodes all over a head that's as well as shorn as mine. Alternatively, a very invasive way where you actually remove the scalp and you lay these things on the cortex itself, correct? Yeah. So the range would be EEG, which is where sensors are placed on the scalp directly, recording non-invasive weight. You can remove them at any time. And then the far other extreme is electrodes that are actually placed into the brain. So with ECOG, tell me how many words per minute you could capture from a patient with ALS. OK. So what we did in our clinical trial at UCSF? Was this the 2023 paper? Yes. OK. This is the nature paper. That's right. So we published a paper in 2023. We worked with an participant named Anne. She had a very severe brainstem stroke about 20 years ago. How old was Anne? She was in her 20s. It wasn't long after she had gone married. Actually, just a couple months after her second daughter was born. She was playing volleyball with her friends, collapsed, taken into the hospital. She survived the injury. She thought that was at a her T-blarter rease. That's a dissection. Yeah. OK. Peter, you're very, this is just a story. You're just from medical schools actually pretty good. This is impressive. Anne survived this stroke. She was left quadriplegic, meaning she couldn't move her arms or legs. But in addition to that, she couldn't speak. Because the nerves that come through the brain down through the brainstem and go to the cranial nerves would supply the vocal tract. Those were also directly affected. And was the paralysis or result of her cerebellum? Also having in-farks? No. It was all brainstem. Just brainstem. In the pruca. Yeah, precisely the part that we call the ponds. So, yeah. So, devastating. Devasting. So it was for 20 years and is now in her 40s in a wheelchair unable to speak. Right. And so what we did was we did a surgery where we implanted an array of 253 ECOG sensors. These are the sensors that are densely spaced. How many? 253. Right. So, we're talking about something about the surface area of a credit card. And it's filled with electro-ed sensors that are spaced about three millimeters apart. Each sensor is about a millimeter and diameter. And so basically you've got this credit card sized array that was placed on the part of a brain that processes words. In particular, the motor production of the words. The parts that control the lips, the jaw, the larynx, the tongue. Eries that were functionally disconnected from her vocal tract because of this stroke and the brainstem, which connects the brain to those muscles. And so we did the surgery about three weeks later. We started our research sessions with her. We connected the cable. It's basically an HDMI cable that is attached to a head stage. The head stage transforms the analog signals from her brain. These are a small voltage. At the same time that we're doing all of this research Peter, AI is developing in parallel. All of these tools that we now are using every day, transcribing our voice right now into text. We use that technology. We can actually use those same technologies that generate voices of speech synthesis. And so we've used a lot of the same tools, machine learning tools that are in modern day AI's. And we're now applying them on the brain activity and trying to use them to not translate, for example, text and synthesize speech. But now the equation is different. It's translate from brain activity into synthesize speech. The input is not text. The input is the ECOG activity across these 253 sensors. For days, what we would do is have a sentence on a screen. And we'd give her the start time and time. And during that, she would just look at the sentence. She could go to go cue and then try to say it. Nothing intelligible comes out. She may or may not be moving the lips job. But just try to say it. And that turns out to be very important. Oh my God. You can't just think about it. You can just hold it. Wow. You have to actually try to say it. And that's what she did. So we started with a very simple vocabulary of about 27 words. The words that we chose are the NATO code words. Alpha, probably, delta, echo. We did that because we could measure basically the accuracy of the decoder that was analyzing those brain signals and translating them into those 26 different code words. And on the first day, we were able to train the algorithm on a data set of maybe about an hour and a half to get to about 50% accuracy. Does 50% accuracy mean she could get half of them right or anytime you showed her one, there was a 50% chance it would be correct. Both. Well, you just said in our sense identical. Like so she's looking and then trying to say. There was a bias towards the subset of them that she was always getting right and others that she was actually there. Actually, there was a bias. Okay. Yeah. Actually, if I get into details, yes. Some of them were more discriminable than others. And was it based on the number of syllables? Yes. Yes. Actually, it was based on that in some of the phonetic properties. But one of the reasons why NATO code words for us was a really useful training, training task for us is because NATO code words were developed in the first place by the military improved communication accuracy. The reason why we actually use those code words is because sometimes if you just say a, the, you know, like D, Z, there's a lot of confusion. So that's why we actually use those code words. It increases the discriminability and then tells us really work. A lot of those settings you just can't make those errors. For example, you know, pilots. Yep. You know, in the column numbers, for example. So we use that because it has high discrimability and on that first day, you know, I think we've got about 50%, and this is going straight to voice. No, this was going straight. This is going to text. This is straight to text. Okay. Yeah. And so we're just trying to figure out, could we, um, decode which word it was and it was displayed in text? Um, so that's pretty, me. That's the first day. That's the first day. Yeah. And then over the next, I would say about six days, uh, the performance just got better and better. And then by about, you know, like a weekend to this, she was up into the 95% range. Um, so that was unexpected. Um, there's incredible to see the performance increase, you know, so quickly. But that did take a full week to do that. Did you get a sense from Anne as to how her level of fatigue with this progressed? In other words, what becomes the bottleneck? Is it, does it get easier and easier for her to go through this talking motion as she practices more? Is it just like any other muscle that we think of that has sort of atrophied? And now she's sort of getting her talking back in shape. Yeah. It is a bit of that. And, um, we're trying to make that easier over time. I think in the beginning days, we were trying everything to get her to work. And, um, a lot of it, again, it has to do with this volitional intent to speak. That turns out to be the most critical thing. But one of the things that I thought was really interesting also was we were doing so much, um, decoding through these tasks that over time actually a couple months into this. And it reported to us actually the strength of our oral facial muscles through her job. The time. They were actually getting stronger through this constant therapy, constant rehabilitation. And so I think right now everything is about just decoding the brain activity, you know, two, an artificial, digital thing. But I do think that in the future BCIs are also going to be a way that we can do rehabilitation. Right? Like it's a way that we have this direct readout of what the brain is trying to do. You can essentially build a prosthetic that helps people speak. But in the process, someone who has been spoken for a while, regain some of that natural strength over time. So that's a new indication that we're thinking about in the future. How to use this technology actually to augment an accelerated rehabilitation. If Anne had that stroke today, how different if at all would this process look, if you were working with a person who hadn't spent 20 years or 18 years without speaking? Um, there's no question that I think it would work faster. There's less to learn. For her, not speaking for 18 years, basically meant that she basically had to relearn how to speak. And we had to keep up with her relearning. Her brain was probably re-organizing, relearning actually some of those fundamental things. And she could see the feedback of essentially whether or not she was trying to say was right or wrong. And it was very intense work. So we're trying to make that easier over time. But I think certainly the more preserved, the more recent, you know, that activity is, those memories, the synapses we talked about earlier, the more stable, the more functional they are, the easier it is to actually decode them. So what will be this ceiling for the current technology? How many words per minute? And at what resolution or accuracy do you think the current technology with, because this was ECOG in her case, correct? Where do you think it's going to go? Where do you think it, where will this asymptote? I'm Peter Atia. This podcast relies exclusively on premium subscribers for support, which allows us to provide all our content without taking a single penny from advertisers. I believe this keeps my content honest, making it a trusted resource for listeners like you. As a premium member, you'll get immediate access to our entire back catalog of AMA episodes and all future AMA episodes. You'll get longevity, focused premium articles packed with actionable insights. You'll get unrivaled show notes for each and every episode of the drive, every topic, every study, every resource from each episode carefully curated for you. You'll get quarterly podcast summaries where you'll learn my biggest personal takeaways from the previous 90 days of expert guest episodes and much more. This journey doesn't have to be navigated alone. We can take these steps towards a better, longer life together. Become a premium member today. A Peter Atia MD.com forward slash subscribe to join me in a shared commitment to a healthier future.